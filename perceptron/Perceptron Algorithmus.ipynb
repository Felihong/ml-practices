{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir implementieren den Perzeptron-Algorithmus, und wenden den an den Iris-Datensatz, um eine binäre Klassifikation zu bekommen, damit wir die Blumen zwischen Klassen \"Iris-setosa\" und \"Iris- versicolor\" bzw. \"Iris-setosa\" und \"Iris-virginica\" unterscheiden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def file_get_contents(filename):\n",
    "    return np.genfromtxt(filename, delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daten und Klassen vorbereiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "originaldata = file_get_contents('resources/iris.data')\n",
    "setosa = originaldata[ :50]\n",
    "versicolor = originaldata[50: 100]\n",
    "virginica = originaldata[100: ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nehmen wir 20% zufälligen Dateien als Testdaten, und die restliche als Trainingdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_testdata(data):\n",
    "    length = int(0.2 * len(data))\n",
    "    return data[:length]\n",
    "\n",
    "\n",
    "def get_trainingdata(data):\n",
    "    length = int(0.8 * len(data))\n",
    "    return data[:length]\n",
    "\n",
    "\n",
    "def get_initial(data1, data2):\n",
    "    for i in range(0, len(data1)):\n",
    "            data1[i][len(data1[i])-1] = 1\n",
    "\n",
    "    for j in range(0, len(data2)):\n",
    "        data2[j][len(data2[j]) - 1] = -1\n",
    "\n",
    "    data = np.concatenate((data1, data2), axis=0)\n",
    "    np.random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "test = get_testdata(get_initial(setosa, versicolor))\n",
    "training = get_trainingdata(get_initial(setosa, versicolor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_weight(x):\n",
    "\n",
    "    length = len(x)-1\n",
    "    y = x[length]\n",
    "\n",
    "    w = np.random.randint(10, size=(length, 1))\n",
    "    tmp = np.dot(y, np.transpose(w))\n",
    "\n",
    "    return np.dot(tmp, np.array(x[:-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hier berechnen wir Gewicht Vektor w und bias b, zu ersr wird w und b zufällig initialisiert, danach wird es immer \n",
    "aktualisiert jedes Mal wenn es einen falsch gelabled sample gibt, es wird so lange wiederholt bis alle Samples richtig gelabled sind. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit Perzeptron Algorithmus wird die alle falsch gelabeled Datensätze \"extrem\" mehr gelernt, durch die Veränderung der demgegende Gewichte, die schon richtig gelabeled Datensätze können wir schon relativ leicht ignorieren.\n",
    "Wir wiederholen die \"adjusting\" Phase so lange, bis alle Datensätze richtig klassifiziert werden. \n",
    "(Nur wenn die Daten linear separierbar sind, ansonsten müssen wir mit einer festlegten Iterationsanzahl lernenn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_checked(w, b, training):\n",
    "    for i in range(0, len(training)):\n",
    "        x = training[i]\n",
    "        if (x[-1] == 1.0):\n",
    "            if ((np.dot(w, x) + b) < 0):\n",
    "                return False\n",
    "        elif (np.dot(w, x) >= 0):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def update(w, b ,x):\n",
    "    y = x[-1]\n",
    "    w = np.add(w, 0.95*(np.dot(y, x)))\n",
    "    b = b + 0.95*y\n",
    "    return w, b\n",
    "\n",
    "\n",
    "def get_weight(training):\n",
    "    length = len(training)\n",
    "    column = np.ones((len(training), 1))\n",
    "    training = np.concatenate((column, training), axis=1)\n",
    "\n",
    "    w = np.transpose(np.random.randint(10, size=(len(training[0]-1), 1)))\n",
    "    b = 0\n",
    "\n",
    "    while(all_checked(w, b, training) == False): #sobald es noch falsche gelabled Sample gibt, widerholt es\n",
    "\n",
    "        i = np.random.randint(0, length)\n",
    "        x = training[i]\n",
    "        if (x[-1] == 1.0 and (np.dot(w, x) + b) < 0): # Produkt mit w negativ aber positiv getagged Fall\n",
    "\n",
    "            w = update(w, b, x)[0]\n",
    "            b = update(w, b, x)[1]\n",
    "        if (x[-1] == -1.0 and (np.dot(w, x) + b) > 0):\n",
    "        # mit anderem Fall wandeln wir es zu erst ins positive Sample, damit wir die selbe update Funktion benutzen können\n",
    "            x = np.dot(-1, x)\n",
    "\n",
    "            w = update(w, b, x)[0]\n",
    "            b = update(w, b, x)[1]\n",
    "        if ((np.dot(w, x) + b) == 0):\n",
    "            pass\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " mit update Funktion wird das Gewicht Vektor und bias immer aktualisiert, w = w + ηy(i)x(i), b = b + ηy(i)\n",
    "0.95 entspricht hier die learning rate, welches im Interval (0, 1) liegt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definieren wir jetzt eine Error Funktion, welche die Error Rate berechnet, und schauen wir uns mal das Lernergebnis an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error rate between Iris-setosa and Iris-virginica is 0.0 %\n",
      "error rate between Iris-setosa and Iris-versicolor is 0.0 %\n"
     ]
    }
   ],
   "source": [
    "def error_rate(data1, data2): # kriegen Error rate mit linear separierbaren Datensätzen\n",
    "    error = 0\n",
    "    data = get_initial(data1, data2)\n",
    "    training = get_trainingdata(data)\n",
    "    column = np.ones((len(get_testdata(data)), 1))\n",
    "    test = np.concatenate((column, get_testdata(data)), axis=1)\n",
    "\n",
    "    weight = get_weight(training)[0]\n",
    "    bias = get_weight(training)[1]\n",
    "\n",
    "    for i in range(0, len(test)):\n",
    "        if ((np.dot(weight, test[i]) + bias) >= 0 and (test[i][-1] == -1.0)):\n",
    "            error += 1\n",
    "        elif (np.dot(weight, test[i])+ bias) < 0 and (test[i][-1] == 1.0):\n",
    "            error += 1\n",
    "    return (error/len(test))*100\n",
    "\n",
    "\n",
    "print('error rate between Iris-setosa and Iris-virginica is', error_rate(setosa, virginica), '%')\n",
    "print('error rate between Iris-setosa and Iris-versicolor is', error_rate(setosa, versicolor), '%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
